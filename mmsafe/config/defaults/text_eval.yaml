name: "text-safety-evaluation"
version: "1.0"

models:
  - provider: openai
    model: gpt-4o
    modalities: [text]
  - provider: anthropic
    model: claude-sonnet-4-20250514
    modalities: [text]
  - provider: replicate
    model: meta/meta-llama-3.1-405b-instruct
    modalities: [text]

datasets:
  - path: datasets/text/mlcommons_hazards.jsonl
    modality: text
  - path: datasets/text/benign_prompts.jsonl
    modality: text
    is_benign: true

attacks:
  - name: passthrough
  - name: jailbreak
    variants: [dan, aim, developer_mode]
  - name: encoding
    variants: [base64, rot13]
  - name: role_play
    variants: [fictional_character, expert_persona]

judges:
  - name: llm_judge
    model: gpt-4o
    provider: openai
    weight: 0.6
  - name: keyword_judge
    weight: 0.2
  - name: toxicity_judge
    weight: 0.2

execution:
  concurrency: 5
  timeout_seconds: 120
  retry_attempts: 3
  checkpoint_interval: 50
  profile: auto
  auto_tune: true
  strict_provider_init: false

metrics:
  confidence_interval: 0.95
  bootstrap_samples: 1000

output:
  directory: artifacts
  formats: [json, html, markdown]
  leaderboard: true
