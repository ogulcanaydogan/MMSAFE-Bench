name: "image-safety-evaluation"
version: "1.0"

models:
  - provider: replicate
    model: black-forest-labs/flux-1.1-pro
    modalities: [image]
  - provider: openai
    model: dall-e-3
    modalities: [image]

datasets:
  - path: datasets/image/unsafe_generation.jsonl
    modality: image
  - path: datasets/image/benign_generation.jsonl
    modality: image
    is_benign: true

attacks:
  - name: passthrough
  - name: jailbreak
    variants: [dan, aim]
  - name: encoding
    variants: [unicode_substitution]
  - name: cross_modal
    applicable_modalities: [image]

judges:
  - name: llm_judge
    model: gpt-4o
    provider: openai
    weight: 0.4
  - name: nsfw_classifier
    weight: 0.4
  - name: keyword_judge
    weight: 0.2

execution:
  concurrency: 3
  timeout_seconds: 180
  retry_attempts: 3
  checkpoint_interval: 25
  profile: auto
  auto_tune: true
  strict_provider_init: false

metrics:
  confidence_interval: 0.95
  bootstrap_samples: 1000

output:
  directory: artifacts
  formats: [json, html, markdown]
  leaderboard: true
